{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Course (980)\n",
    "## Assignment Three \n",
    "\n",
    "__Assignment Goals:__\n",
    "\n",
    "- Implementing RNN based language models.\n",
    "- Implementing and applying a Recurrent Neural Network on text classification problem using TensorFlow.\n",
    "- Implementing __many to one__ and __many to many__ RNN sequence processing.\n",
    "\n",
    "In this assignment, you will implement RNN-based language models and compare extracted word representation from different models. You will also compare two different training methods for sequential data: Truncated Backpropagation Through Time __(TBTT)__ and Backpropagation Through Time __(BTT)__. \n",
    "Also, you will be asked to apply Vanilla RNN to capture word representations and solve a text classification problem. \n",
    "\n",
    "\n",
    "__DataSets__: You will use two datasets, an English Literature for language model task (part 1 to 4) and 20Newsgroups for text classification (part 5). \n",
    "\n",
    "\n",
    "1. (30 points) Implement the RNN based language model described by Mikolov et al.[1], also called __Elman network__ and train a language model on the English Literature dataset. This network contains input, hidden and output layer and is trained by standard backpropagation (TBTT with τ = 1) using the cross-entropy loss. \n",
    "   - The input represents the current word while using 1-of-N coding (thus its size is equal to the size of the vocabulary) and vector s(t − 1) that represents output values in the hidden layer from the previous time step. \n",
    "   - The hidden layer is a fully connected sigmoid layer with size 500. \n",
    "   - Softmax Output Layer to capture a valid probability distribution.\n",
    "   - The model is trained with truncated backpropagation through time (TBTT) with τ = 1: the weights of the network are updated based on the error vector computed only for the current time step.\n",
    "   \n",
    "   Download the English Literature dataset and train the language model as described, report the model cross-entropy loss on the train set. Use nltk.word_tokenize to tokenize the documents. \n",
    "For initialization, s(0) can be set to a vector of small values. To improve performance, you can merge all words that occur less often than a threshold (here 3) into a special rare token (\\__unk__). Note that we are not interested in the *dynamic model* mentioned in the original paper. \n",
    "To make the implementation simpler you can use Keras to define neural net layers, including Keras.Embedding. (Keras.Embedding will create an additional mapping layer compared to the Elman architecture.) \n",
    "\n",
    "2. (20 points) TBTT has less computational cost and memory needs in comparison with *backpropagation through time algorithm (BTT)*. These benefits come at the cost of losing long term dependencies [2]. Now let's try to investigate computational costs and performance of learning our language model with BTT. For training the Elman-type RNN with BTT, one option is to perform mini-batch gradient descent with exactly one sentence per mini-batch. (The input  size will be [1, Sentence Length]). \n",
    "\n",
    "    1. Split the document into sentences (you can use nltk.tokenize.sent_tokenize).\n",
    "    2. For each sentence, perform one pass that computes the mean/sum loss for this sentence; then perform a gradient update for the whole sentence. (So the mini-batch size varies for the sentences with different lengths). You can truncate long sentences to fit the data in memory. \n",
    "    3. Report the model cross-entropy loss.\n",
    "\n",
    "3. (15 points) It does not seem that simple recurrent neural networks can capture truly exploit context information with long dependencies, because of the problem that gradients vanish and exploding. To solve this problem, gating mechanisms for recurrent neural networks were introduced. Try to learn your last model (Elman + BTT) with the SimpleRnn unit replaced with a Gated Recurrent Unit (GRU). Report the model cross-entropy loss. Compare your results in terms of cross-entropy loss with two other approach(part 1 and 2). Use each model to generate 10 synthetic sentences of 15 words each. Discuss the quality of the sentences generated - do they look like proper English? Do they match the training set?\n",
    "    Text generation from a given language model can be done using the following iterative process:\n",
    "   1. Set sequence = \\[first_word\\], chosen randomly.\n",
    "   2. Select a new word based on the sequence so far, add this word to the sequence, and repeat. At each iteration, select the word with maximum probability given the sequence so far. The trained language model outputs this probability. \n",
    "\n",
    "4. (15 points) The text describes how to extract a word representation from a trained RNN (Chapter 4). How we can evaluate the extracted word representation for your trained RNN? Compare the words representation extracted from each of the approaches using one of the existing methods.\n",
    "\n",
    "5. (20 points) We are aiming to learn an RNN model that predicts document categories given its content (text classification). For this task, we will use the 20Newsgroupst dataset. The 20Newsgroupst contains messages from twenty newsgroups.  We selected four major categories (comp, politics, rec, and religion) comprising around 13k documents altogether. Your model should learn word representations to support the classification task. For solving this problem modify the __Elman network__ architecture such that the last layer is a softmax layer with just 4 output neurons (one for each category). \n",
    "\n",
    "    1. Download the 20Newsgroups dataset, and use the implemented code from the notebook to read in the dataset.\n",
    "    2. Split the data into a training set (90 percent) and validation set (10 percent). Train the model on  20Newsgroups.\n",
    "    3. Report your accuracy results on the validation set.\n",
    "\n",
    "__NOTE__: Please use Jupyter Notebook. The notebook should include the final code, results and your answers. You should submit your Notebook in (.pdf or .html) and .ipynb format. (penalty 10 points) \n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "The university policy on academic dishonesty and plagiarism (cheating) will be taken very seriously in this course. Everything submitted should be your own writing or coding. You must not let other students copy your work. Spelling and grammar count.\n",
    "\n",
    "Your assignments will be marked based on correctness, originality (the implementations and ideas are from yourself), clarification and test performance.\n",
    "\n",
    "\n",
    "[1] Tom´ as Mikolov, Martin Kara ˇ fiat, Luk´ ´ as Burget, Jan ˇ Cernock´ ˇ y,Sanjeev Khudanpur: Recurrent neural network based language model, In: Proc. INTERSPEECH 2010\n",
    "\n",
    "[2] Tallec, Corentin, and Yann Ollivier. \"Unbiasing truncated backpropagation through time.\" arXiv preprint arXiv:1705.08209 (2017).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:talk.politics.misc\n",
      "Category:talk.politics.mideast\n",
      "Category:talk.religion.misc\n",
      "Category:comp.windows.x\n",
      "Category:soc.religion.christian\n",
      "Category:rec.motorcycles\n",
      "Category:rec.autos\n",
      "Category:talk.politics.guns\n",
      "Category:comp.graphics\n",
      "Category:comp.sys.ibm.pc.hardware\n",
      "Category:rec.sport.baseball\n",
      "Category:comp.os.ms-windows.misc\n",
      "Category:rec.sport.hockey\n",
      "Category:comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This code is used to read all news and their labels\"\"\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def to_categories(name, cat=[\"politics\",\"rec\",\"comp\",\"religion\"]):\n",
    "    for i in range(len(cat)):\n",
    "        if str.find(name,cat[i])>-1:\n",
    "            return(i)\n",
    "    print(\"Unexpected folder: \" + name) # print the folder name which does not include expected categories\n",
    "    return(\"wth\")\n",
    "\n",
    "def data_loader(images_dir):\n",
    "    categories = os.listdir(data_path)\n",
    "    news = [] # news content\n",
    "    groups = [] # category which it belong to\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(\"Category:\"+cat)\n",
    "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
    "            news.append(open(the_new_path,encoding = \"ISO-8859-1\", mode ='r').read())\n",
    "            groups.append(cat)\n",
    "\n",
    "    return news, list(map(to_categories, groups))\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"datasets/20news_subsampled\"\n",
    "news, groups = data_loader(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "English_literature_path = './datasets/English Literature.txt'\n",
    "English_literature = open(English_literature_path).read()\n",
    "\n",
    "# print(English_literature[:100])\n",
    "\n",
    "# UW = Counter(English_literature)\n",
    "# print(UW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "English_sentences = nltk.sent_tokenize(English_literature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:\\nBefore we proceed any further, hear me speak.']\n"
     ]
    }
   ],
   "source": [
    "print(English_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "## *unk* is used for words with frequency less than 3\n",
    "\n",
    "## 17% accuracy achieved after 5 epoches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Citizen', ':', 'Before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.']\n"
     ]
    }
   ],
   "source": [
    "English_words = nltk.word_tokenize(English_sentences[0])\n",
    "print(English_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Citizen', ':', 'Before', 'we']\n"
     ]
    }
   ],
   "source": [
    "English_words = nltk.word_tokenize(English_literature)\n",
    "print(English_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14309\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "unique_words =  Counter(English_words)\n",
    "\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254533\n",
      "4243\n"
     ]
    }
   ],
   "source": [
    "# print(English_words[:50])\n",
    "\n",
    "for i in range(0,len(English_words)):\n",
    "    \n",
    "    if unique_words[English_words[i]] <=3:\n",
    "        English_words[i] = \"*unk*\";\n",
    "\n",
    "# print(English_words[:50])\n",
    "\n",
    "\n",
    "unique_words_new =  Counter(English_words)\n",
    "\n",
    "# print((unique_words_new))\n",
    "\n",
    "print(len(English_words))\n",
    "print(len(unique_words_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Citizen', ':', 'Before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.', 'All', ':', 'Speak', ',', 'speak', '.', 'First']\n",
      "[286 192]\n",
      "(254533,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab = sorted(set(English_words))\n",
    "word_2_id = {u:i for i, u in enumerate(vocab)}\n",
    "id_2_word = np.array(vocab)\n",
    "w2i1 = word_2_id\n",
    "English_word_as_id = np.array([word_2_id[c] for c in English_words])\n",
    "print(English_words[:20])\n",
    "print(English_word_as_id[:2])\n",
    "\n",
    "# print(word_2_id[English_words[0]])\n",
    "print(English_word_as_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254532, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequence = []\n",
    "\n",
    "for i in range (0,len(English_word_as_id)-1):\n",
    "    \n",
    "#     input_data.append([English_word_as_id[i]])\n",
    "#     output_data.append(English_word_as_id[i+1])\n",
    "    \n",
    "    sequence.append([[English_word_as_id[i]],[English_word_as_id[i+1]]])\n",
    "    \n",
    "print(np.array(sequence).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254532\n",
      "4243\n",
      "254533\n"
     ]
    }
   ],
   "source": [
    "# print(input_data)\n",
    "# print(sequence[:5])\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "inp = (np.array(sequence))\n",
    "input_data = inp[:,0]\n",
    "output_data = inp[:,1]\n",
    "# print(inp[:5])\n",
    "outp = to_categorical(output_data)\n",
    "\n",
    "# print(inp[:5])\n",
    "# print(outp[:5])\n",
    "print((len(input_data)))\n",
    "print(len(outp[1,:]))\n",
    "print(len(English_word_as_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(outp[1,:])\n",
    "embedding_dim = 256\n",
    "rnn_units = 500\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    \n",
    "    tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                        activation = 'sigmoid',\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    \n",
    "    tf.keras.layers.Dense(vocab_size, activation = 'softmax')\n",
    "  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 256)         1086208   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 500)               378500    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4243)              2125743   \n",
      "=================================================================\n",
      "Total params: 3,590,451\n",
      "Trainable params: 3,590,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 254532 samples\n",
      "Epoch 1/5\n",
      "254532/254532 [==============================] - 36s 142us/sample - loss: 5.7192 - acc: 0.1185\n",
      "Epoch 2/5\n",
      "254532/254532 [==============================] - 36s 142us/sample - loss: 5.1802 - acc: 0.1546\n",
      "Epoch 3/5\n",
      "254532/254532 [==============================] - 36s 143us/sample - loss: 4.9908 - acc: 0.1631\n",
      "Epoch 4/5\n",
      "254532/254532 [==============================] - 36s 142us/sample - loss: 4.8743 - acc: 0.1683\n",
      "Epoch 5/5\n",
      "254532/254532 [==============================] - 36s 141us/sample - loss: 4.7945 - acc: 0.1709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f959c79a8d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_data, outp, batch_size = 500, epochs=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some prediction with the model\n",
    "\n",
    "The model predicts unknown words and tried to mimic the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First']\n",
      "['Servant']\n",
      "[':']\n",
      "['I']\n",
      "['have']\n",
      "['*unk*']\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('modelQ1')\n",
    "\n",
    "w_test = \"First\"\n",
    "input_test = word_2_id[w_test]\n",
    "\n",
    "# print(input_test)\n",
    "print([w_test])\n",
    "pre_word = model.predict_classes([input_test])\n",
    "\n",
    "for p in range (0,5):\n",
    "    print(id_2_word[pre_word])\n",
    "    \n",
    "    pre_word = model.predict_classes([pre_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "## *unk* is used for words with frequency less than 3\n",
    "\n",
    "## 49.99% accuracy achieved after 5 epoches \n",
    "\n",
    "pad sequence was used for post padding and truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "English_words_from_sentence = []\n",
    "for x in English_sentences:\n",
    "    \n",
    "    word = nltk.word_tokenize(x)\n",
    "#     print(word[1])\n",
    "    \n",
    "    for i in range(0,len(word)):\n",
    "    \n",
    "        if unique_words[word[i]] <=3:\n",
    "            word[i] = \"*unk*\";\n",
    "\n",
    "    English_words_from_sentence.append(word)\n",
    "#     print(English_words)\n",
    "\n",
    "# print(English_words_from_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[287, 193, 41, 127, 4054, 3065, 969, 2011, 37, 2179, 2620, 3551, 39]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# unique_words = Counter(English_words_from_sentence)\n",
    "# print(unique_words[:5])\n",
    "# vocab = sorted(set(English_words_from_sentence))\n",
    "# word_2_id = {u:i for i, u in enumerate(vocab)}\n",
    "# id_2_word = np.array(vocab)\n",
    "# English_words = nltk.word_tokenize(English_literature)\n",
    "vocab = sorted(set(English_words))\n",
    "vocab = np.append(\"PAD\",vocab)\n",
    "word_2_id = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "w2i2 = word_2_id\n",
    "id_2_word = np.array(vocab)\n",
    "\n",
    "English_sentence_2_id = []\n",
    "for sentence in English_words_from_sentence:\n",
    "    sentence_id = []\n",
    "    for word in sentence:\n",
    "        \n",
    "        sentence_id.append(word_2_id[word])\n",
    "        \n",
    "    English_sentence_2_id.append(sentence_id)\n",
    "    \n",
    "    \n",
    "# print(English_words_from_sentence[0])    \n",
    "print(English_sentence_2_id[0])\n",
    "\n",
    "\n",
    "Padded_English_sentence_2_id = pad_sequences(English_sentence_2_id, maxlen = 21, padding = 'post', truncating='post')\n",
    "# print(id_2_word[0])\n",
    "\n",
    "# print(word_2_id[\"Resolved\"])\n",
    "\n",
    "        \n",
    "\n",
    "# English_word_as_id = np.array([word_2_id[c] for c in English_words_from_sentence])\n",
    "\n",
    "\n",
    "\n",
    "# id_2_word= np.append(id_2_word,\"*unk*\")\n",
    "# word_2_id[\"*unk*\"] = len(id_2_word)-1 \n",
    "# # print(word_2_id[\"*unk*\"])\n",
    "\n",
    "# for i in range (0, len(English_words_from_sentence)):\n",
    "    \n",
    "#     if len(English_words_from_sentence[i]) >50:\n",
    "        \n",
    "#         English_words_from_sentence[i]  = English_words_from_sentence[i][:50]\n",
    "        \n",
    "#     else:\n",
    "#         while len(English_words_from_sentence[i]) < 50:\n",
    "#             English_words_from_sentence[i].append(\"*unk*\")\n",
    "            \n",
    "    \n",
    "\n",
    "# print(English_words_from_sentence[1])\n",
    "\n",
    "# English_sentence_2_id = []\n",
    "# for sentence in English_words_from_sentence:\n",
    "#     sentence_id = []\n",
    "#     for word in sentence:\n",
    "        \n",
    "#         sentence_id.append(word_2_id[word])\n",
    "        \n",
    "#     English_sentence_2_id.append(sentence_id)\n",
    "    \n",
    "# print(English_words_from_sentence[1])\n",
    "# print(English_sentence_2_id[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12460, 20)\n",
      "(12460, 20, 4244)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "English_sentence_2_id = np.array(English_sentence_2_id)\n",
    "# print(len(English_sentence_2_id[:]))\n",
    "# English_sentence_2_id = np.reshape(English_sentence_2_id,(len(English_sentence_2_id),20))\n",
    "\n",
    "# inp = (np.array(English_sentence_2_id))\n",
    "input_data = Padded_English_sentence_2_id[:,:-1]\n",
    "print(input_data.shape)\n",
    "output_data = Padded_English_sentence_2_id[:,1:]\n",
    "# print(input_data[1])\n",
    "# print(inp[:5])\n",
    "outp = to_categorical(output_data)\n",
    "print(outp.shape)\n",
    "\n",
    "# print(inp[:5])\n",
    "# print(outp[:5])\n",
    "# print((len(input_data)))\n",
    "# print(len(outp[1,:]))\n",
    "# print(len(English_word_as_id))\n",
    "\n",
    "# print(English_sentence_2_id[1])\n",
    "# output_data = English_sentence_2_id[:,1]\n",
    "# input_data = English_sentence_2_id[:,0]\n",
    "# # output_data = English_sentence_2_id[:][1:]\n",
    "# print(input_data.shape)\n",
    "# print(output_data.shape)\n",
    "# outp = to_categorical(output_data, num_classes = len(vocab)+1)\n",
    "# print(len(input_data[:,1]))\n",
    "# print(len(output_data[:,1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = outp.shape[2]\n",
    "embedding_dim = 256\n",
    "rnn_units = 500\n",
    "modelQ2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,input_length = 20),\n",
    "    \n",
    "    tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                        activation = 'sigmoid',\n",
    "                        return_sequences = True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    \n",
    "    tf.keras.layers.Dense(vocab_size, activation = 'softmax')\n",
    "  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelQ2.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 256)           1086464   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 20, 500)           378500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20, 4244)          2126244   \n",
      "=================================================================\n",
      "Total params: 3,591,208\n",
      "Trainable params: 3,591,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelQ2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12460 samples\n",
      "Epoch 1/5\n",
      "12460/12460 [==============================] - 413s 33ms/sample - loss: 3.4888 - acc: 0.4590\n",
      "Epoch 2/5\n",
      "12460/12460 [==============================] - 414s 33ms/sample - loss: 3.1123 - acc: 0.4765\n",
      "Epoch 3/5\n",
      "12460/12460 [==============================] - 413s 33ms/sample - loss: 2.9157 - acc: 0.4844\n",
      "Epoch 4/5\n",
      "12460/12460 [==============================] - 410s 33ms/sample - loss: 2.7468 - acc: 0.4924 - loss: 2.7474\n",
      "Epoch 5/5\n",
      "12460/12460 [==============================] - 410s 33ms/sample - loss: 2.5974 - acc: 0.4999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96a70ef630>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelQ2.fit(input_data, outp,batch_size = 1, epochs=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Prediction on model 2\n",
    "\n",
    "## Was not meaning full text but didnot produce unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First\n",
      "Servant\n",
      ":\n",
      "I\n",
      "am\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "modelQ2.save_weights('modelQ2')\n",
    "\n",
    "# print(Padded_English_sentence_2_id[0])\n",
    "w_test = [287, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# input_test = word_2_id[w_test]\n",
    "# pad_test_word = pad_sequences(w_test, maxlen = 20, padding = 'post', truncating='post')\n",
    "# print(input_test)\n",
    "print(id_2_word[w_test[0]])\n",
    "pre_word = modelQ2.predict_classes([w_test])\n",
    "# print(pre_word[0])\n",
    "for p in range (0,5):\n",
    "    print(id_2_word[pre_word[0,0]])\n",
    "    \n",
    "    pre_word = modelQ2.predict_classes([pre_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "## data same as Question 2\n",
    "\n",
    "## 65.5% accuracy achieved after 10 epoches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = outp.shape[2]\n",
    "embedding_dim = 256\n",
    "rnn_units = 500\n",
    "modelQ3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,input_length = 20),\n",
    "    \n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        activation = 'sigmoid',\n",
    "                        return_sequences = True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    \n",
    "    tf.keras.layers.Dense(vocab_size, activation = 'softmax')\n",
    "  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelQ3.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 256)           1086464   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 20, 500)           1137000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20, 4244)          2126244   \n",
      "=================================================================\n",
      "Total params: 4,349,708\n",
      "Trainable params: 4,349,708\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelQ3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## I by mistake made the cell of model 3 output as markdown so my original output for 10 epoches vanished at the very last moment. Although I saved the model and I also can rerun the same notebook and send you an image of same accuracy as mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12460 samples\n",
      "Epoch 1/5\n",
      "12460/12460 [==============================] - 722s 58ms/sample - loss: 3.4056 - acc: 0.4628\n",
      "Epoch 2/5\n",
      "12460/12460 [==============================] - 711s 57ms/sample - loss: 2.9826 - acc: 0.4828\n",
      "Epoch 3/5\n",
      "12460/12460 [==============================] - 712s 57ms/sample - loss: 2.7382 - acc: 0.4931\n",
      "Epoch 4/5\n",
      "12460/12460 [==============================] - 714s 57ms/sample - loss: 2.5073 - acc: 0.5040\n",
      "Epoch 5/5\n",
      "12460/12460 [==============================] - 721s 58ms/sample - loss: 2.2779 - acc: 0.5182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f94d8407908>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelQ3.fit(input_data, outp,batch_size = 1, epochs=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some prediction by model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pre_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e75b152a53b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_2_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_2_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpre_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpre_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelQ3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpre_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_word' is not defined"
     ]
    }
   ],
   "source": [
    "modelQ3.save_weights('modelQ3')\n",
    "\n",
    "# print(Padded_English_sentence_2_id[0])\n",
    "w_test = [287, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# input_test = word_2_id[w_test]\n",
    "# pad_test_word = pad_sequences(w_test, maxlen = 20, padding = 'post', truncating='post')\n",
    "# print(input_test)\n",
    "print(id_2_word[w_test[0]])\n",
    "for p in range (0,5):\n",
    "    print(id_2_word[pre_word[0,0]])\n",
    "    \n",
    "    pre_word = modelQ3.predict_classes([pre_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 \n",
    "\n",
    "## Euclidean similarity was used.\n",
    "\n",
    "## Embedding data from the first layer of every network  was taken\n",
    "\n",
    "## Results shows Network 3 to be best by intrinsic evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Model->1---------------------\n",
      "GOD, Good \n",
      "[[0.254629]]\n",
      "Affection, love \n",
      "[[0.42411244]]\n",
      "love, like \n",
      "[[0.33480805]]\n",
      "above, under \n",
      "[[0.4154392]]\n",
      "----------------------Model->2---------------------\n",
      "GOD, Good \n",
      "[[0.16213137]]\n",
      "Affection, love \n",
      "[[0.23802963]]\n",
      "love, like \n",
      "[[0.19777091]]\n",
      "above, under \n",
      "[[0.2101234]]\n",
      "----------------------Model->3---------------------\n",
      "father, mother \n",
      "[[0.60517615]]\n",
      "king, queen \n",
      "[[0.6159821]]\n",
      "love, like \n",
      "[[0.5884967]]\n",
      "east, west \n",
      "[[0.5954269]]\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "data = model.layers[0].get_weights()[0]\n",
    "data2 = modelQ2.layers[0].get_weights()[0]\n",
    "data3 = modelQ3.layers[0].get_weights()[0]\n",
    "\n",
    "embed1 = dict()\n",
    "for w, i in w2i1.items():\n",
    "    embed1[w] = data[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"----------------------Model->1---------------------\")\n",
    "\n",
    "print(\"GOD, Good \")\n",
    "print((1+euclidean_distances([embed1['God']], [embed1['Good']]))**(-1))\n",
    "\n",
    "print(\"Affection, love \")\n",
    "print((1+euclidean_distances([embed1['Affection']], [embed1['love']]))**(-1))\n",
    "\n",
    "print(\"love, like \")\n",
    "print((1+euclidean_distances([embed1['love']], [embed1['like']]))**(-1))\n",
    "\n",
    "print(\"above, under \")\n",
    "print((1+euclidean_distances([embed1['above']], [embed1['under']]))**(-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embed2 = dict()\n",
    "for w, i in w2i2.items():\n",
    "    embed2[w] = data2[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"----------------------Model->2---------------------\")\n",
    "\n",
    "print(\"GOD, Good \")\n",
    "print((1+euclidean_distances([embed2['God']], [embed2['Good']]))**(-1))\n",
    "\n",
    "print(\"Affection, love \")\n",
    "print((1+euclidean_distances([embed2['Affection']], [embed2['love']]))**(-1))\n",
    "\n",
    "print(\"love, like \")\n",
    "print((1+euclidean_distances([embed2['love']], [embed2['like']]))**(-1))\n",
    "\n",
    "print(\"above, under \")\n",
    "print((1+euclidean_distances([embed2['above']], [embed2['under']]))**(-1))\n",
    "\n",
    "\n",
    "\n",
    "embed3 = dict()\n",
    "for w, i in w2i2.items():\n",
    "    embed3[w] = data3[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"----------------------Model->3---------------------\")\n",
    "\n",
    "print(\"father, mother \")\n",
    "print((1+euclidean_distances([embed3['father']], [embed3['mother']]))**(-1))\n",
    "\n",
    "print(\"king, queen \")\n",
    "print(1/(1+euclidean_distances([embed3['king']], [embed3['queen']])))\n",
    "\n",
    "print(\"love, like \")\n",
    "print(1/(1+euclidean_distances([embed3['love']], [embed3['like']])))\n",
    "\n",
    "print(\"east, west \")\n",
    "print(1/(1+euclidean_distances([embed3['east']], [embed3['west']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "## Results show 72% accuracy in the model after 30 epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:talk.politics.misc\n",
      "Category:talk.politics.mideast\n",
      "Category:talk.religion.misc\n",
      "Category:comp.windows.x\n",
      "Category:soc.religion.christian\n",
      "Category:rec.motorcycles\n",
      "Category:rec.autos\n",
      "Category:talk.politics.guns\n",
      "Category:comp.graphics\n",
      "Category:comp.sys.ibm.pc.hardware\n",
      "Category:rec.sport.baseball\n",
      "Category:comp.os.ms-windows.misc\n",
      "Category:rec.sport.hockey\n",
      "Category:comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This code is used to read all news and their labels\"\"\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def to_categories(name, cat=[\"politics\",\"rec\",\"comp\",\"religion\"]):\n",
    "    for i in range(len(cat)):\n",
    "        if str.find(name,cat[i])>-1:\n",
    "            return(i)\n",
    "    print(\"Unexpected folder: \" + name) # print the folder name which does not include expected categories\n",
    "    return(\"wth\")\n",
    "\n",
    "def data_loader(images_dir):\n",
    "    categories = os.listdir(data_path)\n",
    "    news = [] # news content\n",
    "    groups = [] # category which it belong to\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(\"Category:\"+cat)\n",
    "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
    "            news.append(open(the_new_path,encoding = \"ISO-8859-1\", mode ='r').read())\n",
    "            groups.append(cat)\n",
    "\n",
    "    return news, list(map(to_categories, groups))\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"datasets/20news_subsampled\"\n",
    "news, groups = data_loader(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['From', ':', 'gld', '@', 'cunixb.cc.columbia.edu', '(', 'Gary', 'L', 'Dare', ')', 'Subject', ':', 'Re', ':', 'EIGHT', 'MYTHS', 'about', 'National', 'Health', 'Insurance', '(', 'Pt', 'II', ')', 'v140pxgt', '@', 'ubvmsb.cc.buffalo.edu', '(', 'Daniel', 'B', 'Case', ')', 'writes', ':', '>', 'gld', '@', 'cunixb.cc.columbia.edu', '(', 'Gary', 'L', 'Dare', ')', 'writes', '...', '>', '>', 'v140pxgt', '@', 'ubvmsb.cc.buffalo.edu', '(', 'Daniel', 'B', 'Case', ')', 'writes', ':', '>', '>', '>', 'gld', '@', 'cunixb.cc.columbia.edu', '(', 'Gary', 'L', 'Dare', ')', 'writes', '...', '>', '>', 'Okay', ',', 'but', 'do', 'doctors', 'willingly', 'testify', 'against', 'each', 'other', 'in', '>', 'malpractice', 'cases', 'when', 'they', 'do', 'go', 'to', 'court', '(', 'obviously', ',', 'absolutely', '>', 'essential', 'to', 'prove', 'malpractice', ')', '?', 'It', 'used', 'to', 'be', 'impossible', 'to', 'get', '>', 'doctors', 'here', 'to', 'do', 'that', '(', 'A', 'possible', 'advantage', 'of', 'the', 'US', 'system', '>', 'you', 'wo', \"n't\", 'hear', 'about', 'from', 'the', 'AMA', ')', '.', 'Our', 'doctors', \"'\", 'monopoly', 'is', 'exactly', 'the', 'same', 'as', 'in', 'the', 'U.S.', ',', 'if', 'not', 'more', 'powerful', 'now', 'that', 'they', 'can', 'dictate', 'insurance', 'payment', 'rates', ',', 'but', 'I', 'do', \"n't\", 'know', 'an', 'answer', 'to', 'this', 'one', '.', 'Anecdotally', ',', 'my', 'friends', 'who', 'are', 'MD', \"'s\", '(', 'including', 'my', 'main', 'buds', 'from', 'high', 'school', ')', 'talk', 'about', 'how', 'hard', 'it', 'is', 'to', 'turn', '``', 'state', \"'s\", 'witness', \"''\", 'against', 'someone', 'else', '...', 'no', 'direct', 'experience', 'there', ',', 'though', '.', '>', 'Also', ',', 'in', 'some', 'circumstances', 'you', 'may', 'have', 'to', 'sue', 'the', 'insurance', 'plan-', '>', 'people', 'here', ',', 'after', 'all', ',', 'sue', 'health', 'insurance', 'companies', 'all', 'the', 'time', '.', '>', 'I', 'heard', 'about', 'a', 'guy', 'in', 'Alberta', 'who', 'came', 'down', 'with', 'some', 'rare', 'eye', '>', 'disease', 'that', 'he', 'had', 'to', 'take', 'repeated', 'trips', 'to', 'Seattle', 'to', 'get', 'treated', '.', '>', 'It', 'cost', 'him', 'and', 'his', 'family', 'something', 'like', '$', '6000', 'and', 'the', 'province', ',', '>', 'years', 'later', ',', 'still', 'has', 'only', 'reimbursed', 'them', 'for', '$', '500', 'or', 'so', '.', 'Well', ',', 'what', 'American', 'private', 'insurance', 'plans', 'cover', 'travel', 'expenses', '?', '?', '?', 'Since', 'our', 'public', 'insurance', 'plans', 'are', 'publicly', 'accountable', ',', 'one', 'can', 'raise', 'a', 'stink', 'in', 'the', 'media', 'to', 'try', 'and', 'extort', 'benefits', 'beyond', 'which', 'one', 'is', 'entitled', '(', 'hey', ',', 'not', 'Alberta', \"'s\", 'fault', 'that', 'he', 'lives', 'there', ')', '...', 'If', 'he', 'lived', 'in', 'Cheyenne', ',', 'WY', 'his', 'private', 'insurance', 'would', \"'ve\", 'told', 'him', 'to', 'go', 'to', 'hell', 'for', 'the', 'travel', 'expenses', 'and', 'that', \"'s\", 'that', '.', 'An', 'HMO', 'would', 'have', 'just', 'kept', 'quiet', 'and', 'let', 'him', 'go', 'blind', '.', '>', '>', '>', 'Well', ',', 'yeah', ',', 'tell', 'us', 'about', 'the', 'National', 'Defense', 'Medical', 'Centre', '>', '>', '>', 'outside', 'Ottawa', '.', '>', '>', '>', '>', 'It', 'serves', 'the', 'same', 'purpose', 'as', 'the', 'Bethesda', 'Naval', 'Hospital', '...', 'since', '>', '>', 'not', 'all', 'hospitals', 'can', 'provide', 'everything', ',', 'maybe', 'they', 'have', 'some', 'stuff', '>', '>', 'that', 'others', 'do', \"n't\", '?', '(', 'Ottawa', \"'s\", 'population', 'is', 'only', 'a', 'quarter', 'million', ',', '>', '>', 'if', 'you', 'include', 'the', 'surrounding', 'counties', '.', ')', '>', '>', 'My', 'point', 'was', 'that', 'something', 'that', 'should', 'necessarily', 'remain', '>', 'unpoliticized', 'has', 'become', 'very', 'politicized', ',', 'to', 'the', 'detriment', '>', 'of', 'its', 'mission', '.', 'I', 'do', \"n't\", 'think', 'that', 'this', 'has', 'been', 'shown', 'with', 'the', 'DMC', '...', '>', '>', '>', 'The', 'problem', 'is', ',', 'in', 'a', 'system', 'where', 'hospitals', \"'\", 'annual', 'budgets', 'are', '>', '>', '>', '>', 'approved', 'by', 'the', 'government', ',', 'how', 'do', 'you', 'keep', 'political', 'considerations', '>', '>', '>', 'out', 'of', 'medical', 'decisions', '?', 'I', 'bet', 'that', 'if', 'you', \"'re\", 'an', 'MP', 'or', 'MPP', ',', 'or', 'good', '>', '>', '>', 'friends', 'with', 'one', ',', 'you', \"'re\", 'put', 'on', 'any', 'hospital', \"'s\", '``', 'urgent', \"''\", 'care', 'list', 'no', '>', '>', '>', 'matter', 'how', 'minor', 'your', 'problem', '.', 'Which', 'is', 'OK', 'unless', 'you', \"'re\", 'someone', 'who', '>', '>', '>', 'gets', 'bumped', 'off', 'the', 'list', 'for', 'some', 'bigshot', '.', '>', '>', '>', '>', 'People', 'of', 'influence', 'will', 'get', 'their', 'way', 'in', 'any', 'system', ',', 'American', 'or', '>', '>', 'European', '.', 'It', \"'s\", 'the', '``', 'Golden', 'Rule', \"''\", '-', 'he', 'who', 'has', 'the', 'gold', 'makes', 'the', '>', '>', 'rules', '.', '(', '-', ';', '>', '>', 'But', 'to', 'what', 'extent', 'does', 'it', 'affect', 'the', 'system', '?', 'And', 'why', 'is', 'an', 'urgent', '>', 'care', 'list', 'necessary', 'in', 'the', 'first', 'place', '?', 'It', \"'s\", 'worth', 'thinking', 'about', '.', 'It', \"'s\", 'regular', 'practice', 'in', 'a', 'hospital', 'to', 'figure', 'out', 'who', 'needs', 'to', 'get', 'at', 'what', 'facilities', '.', 'Do', \"n't\", 'Americans', 'have', 'to', 'arrange', 'in', 'advance', 'for', 'operations', 'too', '?', 'I', 'think', 'that', 'there', 'are', 'two', 'standards', 'being', 'applied', 'here', ',', 'and', 'that', 'Canada', 'ca', \"n't\", 'give', 'Beverly', 'Hills-style', 'treatment', 'to', 'everybody', '.', 'It', \"'s\", 'not', 'a', 'big', 'brother', 'list', '...', 'it', \"'s\", 'more', 'like', 'calling', 'around', 'town', 'for', 'a', 'table', 'for', 'dinner', '...', '>', 'Yeah', ',', 'but', 'private', 'nonprofit', 'foundations', 'have', 'to', 'make', 'money', 'somehow', ',', '>', 'especially', 'in', 'the', 'hospital', 'business', '.', 'Yes', ',', 'and', 'the', 'Tories', 'in', 'Ottawa', 'are', 'trying', 'to', 'make', 'them', 'do', 'that', 'rather', 'than', 'hope', 'for', 'a', 'bigger', 'grant', 'from', 'the', 'feds', 'and', 'their', 'province', 'the', 'next', 'time', 'around', '.', 'Whether', 'it', \"'s\", 'using', 'mop', 'a', 'couple', 'of', 'weeks', 'longer', 'or', 'even', 'selling', 'services', 'to', 'Americans', '(', 'remember', ',', 'our', 'system', 'is', 'cash', 'based', 'and', 'since', 'our', 'health', 'care', 'infrastructure', 'is', 'overbuilt', 'except', 'in', 'specialties', 'that', 'require', 'larger', 'populations', 'to', 'generate', 'business', ',', 'why', 'not', '?', 'The', 'alternative', 'is', 'closing', 'unused', 'wards', '...', 'business.', ')', '.', '>', 'whether', 'Canadians', 'would', 'be', 'thrilled', 'at', 'the', 'prospect', 'of', 'their', 'own', '>', 'health', 'services', 'catering', 'toward', 'Americans', ',', 'who', 'would', 'be', 'willing', '>', 'to', 'pay', 'more', 'than', 'they', 'do', ',', 'is', 'another', 'issue', 'entirely', ')', ',', 'it', 'must', 'be', '>', 'noted', 'that', 'they', 'said', 'they', 'were', 'doing', 'it', 'partly', 'because', 'their', 'grants', '>', 'from', 'the', 'province', 'were', 'getting', 'smaller', 'If', 'those', 'grants', 'are', 'so', '>', 'insubstantial', ',', 'why', 'the', 'need', 'to', 'attract', 'foreigners', 'to', 'make', 'up', 'the', '>', 'difference', '?', 'You', 'answered', 'the', 'question', 'yourself', '...', '``', 'private', 'nonprofit', 'foundations', 'have', 'to', 'make', 'money', 'somehow', \"''\", ',', 'and', 'I', 'think', 'that', 'it', \"'s\", 'about', 'time', 'that', 'they', 'acted', 'like', 'the', 'private', 'hospitals', 'that', 'they', 'are', '.', 'Personally', ',', 'I', \"'m\", 'fed', 'up', 'with', 'Canadian', 'socialists', 'trying', 'to', 'tell', 'everyone', 'that', 'their', 'health', 'care', 'is', 'free', 'when', 'we', 'are', 'actually', 'buying', 'insurance', '(', 'that', \"'s\", 'one', 'at', 'you', ',', 'Bob', 'Rae', '!', '!', ')', '.', '>', '>', 'The', 'GDP', 'figures', 'are', 'combined', 'public', 'and', 'private', 'expenditures', 'for', 'total', '>', '>', 'outlay', ',', 'and', 'are', 'compiled', 'use', 'the', 'same', 'methods', 'by', 'the', 'OECD', 'that', 'yield', '>', '>', 'the', '13-14', '%', 'figure', 'for', 'the', 'U.S.', '>', '>', 'But', 'do', \"n't\", 'the', 'US', 'figures', 'include', 'dentistry', 'and', 'optometry', 'where', 'the', '>', 'Canadian', 'one', '(', 'until', 'recently', ',', 'anyway', ')', 'did', \"n't\", '?', 'Since', 'we', 'have', 'always', 'been', 'evaluated', 'in', 'an', 'OECD', 'style', ',', 'I', 'do', \"n't\", 'see', 'how', '...', 'remember', ',', 'OECD', 'counts', 'both', 'private', 'and', 'public', 'funds', ',', 'and', 'in', 'Canada', 'like', 'France', 'and', 'Germany', ',', '30', '%', 'of', 'health', 'care', 'spending', 'is', 'private', 'funds', '(', 'i.e.', ',', 'not', 'the', 'basic', 'health', 'insurance', 'money', ')', '.', '>', '>', '>', 'So', 'what', 'happens', 'if', 'the', 'health', 'care', 'systems', 'financially', 'collapse', '.', '>', '>', '>', '>', 'How', '?', 'They', 'are', 'collecting', 'premiums', '...', 'and', 'I', \"'m\", 'an', 'advocate', 'of', 'having', '>', '>', 'copayments', 'like', 'the', 'French', 'do', 'in', 'their', 'system', '...', '>', '>', 'Well', ',', 'if', 'you', 'spend', 'more', 'than', 'you', 'take', 'in', ',', 'you', 'go', 'bankrupt', '.', 'It', \"'s\", 'that', '>', 'simple', '.', 'If', 'the', 'provincial', 'insurance', 'systems', 'find', 'themselves', 'paying', '>', 'out', 'more', 'than', 'they', 'get', 'in', 'revenue', ',', 'they', 'wo', \"n't\", 'be', 'able', 'to', 'pay', 'for', '>', 'everybody', \"'s\", 'primary', 'care', '.', 'Yes', ',', 'the', 'infrastructure', 'will', 'be', 'there', '.', '>', 'But', 'will', 'everybody', 'be', 'able', 'to', 'continue', 'using', 'it', 'at', 'the', 'same', 'rate', '.', 'Minor', 'copayments', 'can', 'flush', 'out', 'abusers', '.', 'Remember', 'that', 'our', '``', 'system', \"''\", 'is', 'only', 'an', 'insurance', 'policy', '.', 'But', 'our', 'costs', 'are', \"n't\", 'rising', 'fast', 'enough', 'to', 'ensure', 'adequate', 'copayments/deductibles', '...', 'last', 'year', ',', 'Quebec', \"'s\", 'user-fee', 'proposal', 'came', 'out', 'with', 'the', 'number', 'of', '``', '$', '5', \"''\", 'as', 'the', 'necessary', 'hike', 'that', 'could', 'be', 'done', 'through', 'a', 'copayment', 'rather', 'than', 'give', 'the', 'QMA', 'a', 'raise', '.', 'And', 'it', \"'s\", 'not', 'contract', 'time', 'yet', ',', 'as', 'far', 'as', 'I', 'can', 'tell', 'from', 'UPI', 'Clarinet', '...', 'Even', 'the', 'new', 'Reform', 'Party', ',', 'a', 'breakoff', 'of', 'traditionalists', 'from', 'the', 'Conservatives', 'with', 'a', 'mildly', '``', 'libertarian', \"''\", 'faction', ',', 'holds', 'our', 'public', 'health', 'insurance', 'as', 'an', 'untouchable', 'but', 'that', 'just', 'a', 'few', 'people', 'have', 'to', 'be', 'reminded', 'that', 'it', \"'s\", 'not', 'free', '(', 'the', 'average', 'Canadian/European', 'is', 'more', 'fiscally', 'naive', 'than', 'their', 'American', 'counterparts', 'on', 'issues', 'like', 'these', ')', '.', 'But', 'no', 'mention', 'of', 'copayments', 'anywhere', 'to', 'be', 'seen', '...', 'but', 'cutting', 'public', 'spending', 'all', 'over', 'the', 'place', ',', 'and', 'bringing', 'back', 'the', 'death', 'penalty', ',', 'with', 'little', 'haste', 'if', 'elected', '.', '>', 'I', 'know', 'that', ',', 'for', 'Pete', \"'s\", 'sake', ',', 'I', 'live', 'right', 'on', 'the', 'border', '.', 'I', 'know', 'the', '>', 'Canadian', 'system', 'is', \"n't\", 'socialized', 'medicine', '(', 'unlike', 'Britain', \"'s\", 'NHS', ')', '.', 'Sorry', '!', '(', '-', ';', 'It', \"'s\", 'just', 'that', 'I', 'even', 'run', 'into', 'people', 'from', 'Buffalo', 'and', 'from', 'Michigan', 'who', 'do', \"n't\", 'know', '...', '>', 'The', 'point', 'is', ',', 'that', 'means', 'that', 'if', 'the', 'money', 'runs', 'low', 'in', 'the', 'plan', ',', '>', 'you', \"'re\", 'out', 'of', 'luck', 'unless', 'you', 'can', 'afford', 'it', 'yourself', '.', 'Yeah', ',', 'but', 'there', \"'d\", 'be', 'a', 'lot', 'of', 'lead-time', 'and', 'a', 'health-care', 'crisis', 'that', 'would', 'preclude', 'it', '.', 'If', 'provincial', 'governments', '(', 'as', 'bad', 'as', 'some', 'of', 'them', 'are', ';', 'heck', ',', 'we', 'have', 'the', 'NDP', 'cleaning', 'up', 'a', 'spending', 'mess', 'made', 'by', 'the', 'Conservatives', 'in', 'Saskatchewan', '-', 'embarassing', '!', ')', 'can', 'be', 'so', 'irresponsible', ',', 'there', 'is', 'still', 'reallocation', '--', '-', 'health', 'insurance', 'is', 'so', 'important', 'that', 'it', \"'s\", 'about', 'the', 'only', 'thing', 'that', 'can', 'inspire', 'open', 'rebellion', 'and', 'violent', 'insurrection', 'outside', 'of', 'the', 'hockey', 'rink', '.', 'Right', 'now', ',', 'attempts', 'to', 'get', 'the', 'system', 'and', 'its', 'users', 'to', 'learn', 'good', 'habits', 'are', 'being', 'treated', 'like', 'cod-liver', 'oil', '...', '>', '>', '>', 'Would', 'the', 'private', 'insurers', 'take', 'up', 'the', 'slack', '?', 'They', \"'d\", 'be', 'under', 'no', '>', '>', '>', 'obligation', 'to', '.', 'Of', 'course', ',', 'they', 'could', 'eventually', 'make', 'money', 'again', ',', '>', '>', '>', 'but', 'if', 'what', 'you', 'say', 'is', 'true', ',', 'they', \"'d\", 'be', 'loathe', 'to', 'do', 'so', '(', 'and', 'out', 'of', '>', '>', '>', 'practice', 'in', 'handling', 'such', 'basic', 'services', ',', 'too', ')', '.', '>', '>', '>', '>', 'Some', 'of', 'the', 'companies', 'providing', 'extra', 'insurance', 'are', 'subsidiaries', 'of', '>', '>', 'American', 'companies', ',', 'and', 'their', 'parents', 'provide', 'full', 'insurance', 'down', '>', '>', 'here', '.', 'Regardless', ',', 'all', 'firms', 'up', 'north', 'can', 'easily', 'turn', 'on', 'cable', 'TV', '>', '>', 'to', 'see', 'how', 'well', 'the', 'American', 'firms', 'are', 'doing', 'by', 'being', 'involved', 'in', '>', '>', 'basic', 'coverage', '.', 'The', 'private', 'firms', 'are', 'making', 'too', 'much', 'money', 'after', '>', '>', 'having', 'gotten', 'rid', 'of', 'basic', 'coverage', '.', 'They', 'run', 'around', 'patting', 'them-', '>', '>', 'selves', 'on', 'the', 'back', 'for', 'their', 'own', 'cooperation', 'in', 'providing', 'extras', '>', '>', 'for', 'those', 'people', 'who', '``', 'deserve', 'it', \"''\", '.', '>', '>', 'Yeah', ',', 'but', 'eventually', 'it', \"'s\", 'going', 'to', 'create', 'a', 'kind', 'of', 'two-tiered', '>', 'effect', 'that', 'will', 'be', 'noticeable', 'after', 'a', 'while', ',', 'like', 'in', 'Britain', '.', 'Most', 'Americans', 'are', 'fearful', 'of', 'a', 'single-tier', 'system', '...', '(', '-', ';', 'Seriously', ',', 'there', 'are', 'few', 'areas', 'that', 'have', 'sufficient', 'population', 'for', 'a', 'two/more-tiered', 'system', 'like', 'what', 'the', 'French', 'have', '...', 'a', 'health', 'policy', 'prof', ',', 'D.G', '.', 'Shea', ',', 'has', 'cited', 'studies', 'in', 'the', 'NEJM', 'that', 'indicate', 'having', 'a', 'population', 'of', '500,000', 'is', 'necessary', 'for', 'adequate', 'competition', '...', 'and', 'in', 'Canada', ',', 'there', 'are', 'only', 'four', 'cities', 'west', 'of', 'the', 'Great', 'Lakes', 'with', 'that', 'population', 'or', 'larger', '.', 'Anyways', ',', 'the', 'numbers', 'show', 'that', 'costs', 'have', 'held', 'steadier', 'than', 'those', 'in', 'the', 'U.S.', 'and', 'barring', 'any', 'future', 'Chernobyl-like', 'crisis', ',', 'sudden', 'transients', 'in', 'spending', 'are', 'unlikely', '.', 'In', 'fact', ',', 'the', 'health', 'allocation', 'is', 'one', 'of', 'the', 'most', 'well-behaved', 'sectors', 'of', 'spending', 'up', 'north', 'so', 'any', 'talk', 'of', 'bankruptcy', 'is', 'talk-radio', 'fodder', 'far', 'away', 'from', 'the', 'border', '.', '>', 'If', 'the', 'provinces', 'hit', 'fiscal', 'rough', 'spots', 'and', 'have', 'to', 'cut', 'back', ',', 'the', '>', 'things', 'private', 'insurers', 'have', 'to', 'offer', 'will', 'seem', 'less', 'and', 'less', 'like', '>', 'luxuries', 'and', 'the', 'gap', 'will', 'be', 'more', 'and', 'more', 'noticeable', '.', 'This', 'wo', \"n't\", 'be', 'overnight', ',', 'and', 'something', 'like', 'this', 'would', 'force', 'Canada', 'to', 'have', 'a', 'system', 'more', 'like', 'the', 'French', 'one', '...', 'but', 'that', \"'s\", 'not', 'a', 'bad', 'thing', ',', 'and', 'the', 'change', 'will', 'be', 'minimal', '(', 'i.e.', ',', 'add', 'copayments', 'and', 'frustrate', 'the', 'socialists', 'chanting', '``', 'Hey', ',', 'it', \"'s\", '*free*', '!', \"''\", ')', '.', 'gld', '--', '~~~~~~~~~~~~~~~~~~~~~~~~', 'Je', 'me', 'souviens', '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~', 'Gary', 'L.', 'Dare', '>', 'gld', '@', 'columbia.EDU', 'GO', 'Winnipeg', 'Jets', 'GO', '!', '!', '!', '>', 'gld', '@', 'cunixc.BITNET', 'Selanne', '+', 'Domi', '==', '>', 'Stanley']]\n"
     ]
    }
   ],
   "source": [
    "# word = np.array(len(words))\n",
    "# for i in words:\n",
    "#     for j in i:\n",
    "        \n",
    "#         word = np.append(word,j)\n",
    "\n",
    "\n",
    "# type(words)\n",
    "\n",
    "# words[:1]\n",
    "\n",
    "# unique_words =  Counter(words)\n",
    "\n",
    "# print(len(unique_words))\n",
    "# unique_words = [] \n",
    "# for j in words:\n",
    "# unique_words.Counter(words)\n",
    "\n",
    "# print(len(unique_words))\n",
    "words = []\n",
    "for i in news:\n",
    "    words.append(nltk.word_tokenize(i))\n",
    "print(words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336786\n",
      "230172\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "word = list(itertools.chain.from_iterable(words))\n",
    "# print(word[:200])\n",
    "print(len(word))\n",
    "# print(j)\n",
    "unique_words =  Counter(word)\n",
    "\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', ':', 'gld', '@', 'cunixb.cc.columbia.edu', '(', 'Gary', 'L', 'Dare', ')', 'Subject', ':', 'Re', ':', 'EIGHT', 'MYTHS', 'about', 'National', 'Health', 'Insurance']\n",
      "[ 80755  51520 185439  54243 174811   3510  83171  97140  72673   3511\n",
      " 135215  51520 127583  51520  75354 108539 161319 114433  87033  91098]\n",
      "(5336786,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab = sorted(set(word))\n",
    "vocab = np.append(\"PAD\",vocab)\n",
    "word_2_id = {u:i for i, u in enumerate(vocab)}\n",
    "id_2_word = np.array(vocab)\n",
    "\n",
    "word_as_id = np.array([word_2_id[c] for c in word])\n",
    "print(word[:20])\n",
    "print(word_as_id[:20])\n",
    "\n",
    "# print(word_2_id[English_words[0]])\n",
    "print(word_as_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2_id = []\n",
    "\n",
    "\n",
    "for news_sequence in words:\n",
    "    news_id = []\n",
    "    for w in news_sequence:\n",
    "        \n",
    "        news_id.append(word_2_id[w])\n",
    "        \n",
    "    words_2_id.append(news_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(words_2_id[:1])\n",
    "# print(words_2_id[:2])\n",
    "# # print(words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Padded_news_id = pad_sequences(words_2_id, maxlen = 250, padding = 'post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = tf.convert_to_tensor(words_2_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80755,  51520, 163409,  54243, 217491,   3510, 109749,  58101,\n",
       "         3511, 135215,  51520, 211517,  80793, 200607, 209915,  11567,\n",
       "        88143, 178042, 199787, 221259, 221314, 185772, 206664,  11567,\n",
       "        90595, 191726, 177005,   8339, 189425, 191726, 221314, 221667,\n",
       "        11472,   3510,  54244, 211517, 221868,  96865,   3511,   8890,\n",
       "         8890,   8890,   8890,   8890,   8890, 166441, 199696, 211517,\n",
       "         8890,   8890,   8890,   8890,   8890,   8890,   8890,   8890,\n",
       "         8781,  90791, 164614,  52078,  28946,  54243, 196845,  54241,\n",
       "       193414,  54243, 196845,   3510,  96865,  91634,  97017,   3511,\n",
       "       228056,  51520,  54241, 193918,  54243, 223333, 228056,  51520,\n",
       "        54241, 155301,  11567, 157278,  54241,  54241,  54241,  54241,\n",
       "       226889,   8339, 189087, 192774, 177005, 161091, 209042, 197231,\n",
       "       213531, 163394, 169159, 224633, 227599, 221019, 183442, 164614,\n",
       "        54241,  54241, 168792, 138178, 221002, 211778, 221019, 195055,\n",
       "         8339, 189087,   2753, 207606, 217973, 187745,  11567,  54241,\n",
       "        54241,  54241,  73867,  22562,  54241,  54241,  56010,  49414,\n",
       "        11472,  49394,  11472,  54241,  54241,  56588, 138179,  54241,\n",
       "        54241,  89678,  73183, 202326, 110539,   8339, 142087, 202326,\n",
       "        67546,   8339, 132633, 202326, 110539,   8339,  54241,  54241,\n",
       "       100227,  58161,  49391,  11472,  54241,  54241, 137057, 122940,\n",
       "       172322, 202326, 216376, 218990, 161418,  11472,  54241,  54241,\n",
       "       126112, 126094,  56341,  51520,  42211, 125942,  11472, 155301,\n",
       "        11567, 157278,  54241, 148038, 191585, 199787, 221314, 190482,\n",
       "       189900, 221019,  78237, 202623, 196527,  54242,  90595, 226745,\n",
       "       181541,  54241, 204946, 221868, 221259, 226745, 164360, 220654,\n",
       "       221076, 221019, 222891, 161319, 178735,   8339, 227251, 178124,\n",
       "        54241, 221314, 186295, 172980, 210624, 181148, 200259, 221248,\n",
       "       161319, 162931, 162790,  54241, 178735,  11472, 110268, 166351,\n",
       "       221019, 170949, 176531, 210624,  54242, 138549, 161325, 161400,\n",
       "       195037, 225352, 206612, 194971, 221868, 169251, 224812,   3510,\n",
       "       224088, 162931, 186437,   3511, 163394, 195730, 207637,  11472,\n",
       "       134234, 196916, 185192, 203634, 181360, 227089, 221215, 214248,\n",
       "       221002, 164614,   8339, 168703, 227630, 161795, 177102, 224633,\n",
       "       221019, 218758, 163394, 214260, 188648, 221019, 218750, 184854,\n",
       "       221066, 175352, 191726, 209983, 178124, 199787, 220649, 228961,\n",
       "       215217,  11472,  88143,   3043, 185646, 221868, 222292, 178410,\n",
       "       221002, 218758, 188450, 221961,  11472, 155301,  11567, 157278,\n",
       "        54241,  88143, 200546, 170946, 206706, 226586, 198851, 203004,\n",
       "       194521, 175216, 220979,  54241, 170776,  11567,  11567,  88143,\n",
       "       226586, 192774, 222910, 221868, 193047, 203323, 188389,  11472,\n",
       "        90595, 226745, 164360,  54241, 185646, 221868, 179454, 193263,\n",
       "        11567, 163394, 162089,  11567, 163394, 220649, 221076, 221019,\n",
       "       222891, 161319, 178735,  54241, 178042, 199787, 228961, 221259,\n",
       "       161091, 199333, 166744, 164182, 227943, 166270, 221868, 195019,\n",
       "       221019, 207539,   3279, 163394,  54241, 172443,   3279, 202326,\n",
       "       179147, 223279, 202326, 178724,  11472, 147933, 192787, 161016,\n",
       "       222891, 161319, 178735,  54242, 128108,  54242, 147933, 216918,\n",
       "       202326, 211339,  54242,  69768, 175352, 169204, 187674, 180875,\n",
       "       161091, 221134,   8339, 168703, 191726, 178124, 201620, 208108,\n",
       "       163876,  11472,  54241,  87896, 169204, 228961, 181541, 216806,\n",
       "       221868, 196209, 161091,  54241, 175782, 227089, 221019, 118959,\n",
       "       202623, 202664, 186960, 213472, 221002, 162790, 178735, 164360,\n",
       "       161371,  54241,  60296,  60296,  60296,  11567,  11641, 221314,\n",
       "       200676, 172935, 210619, 221868, 162013, 221002,  54241, 178735,\n",
       "       194833, 187791, 181093, 187342, 200259, 215503, 179540,  11472,\n",
       "        87292, 160827, 161091, 210836, 213098, 178724,  11472, 147933,\n",
       "       196228, 191726, 224465, 164360,  90298, 161988, 163394, 215217,\n",
       "       194833, 162080,  11472, 138608, 164360, 215503, 179540,   8339,\n",
       "       194833, 227610,   8339, 168703, 221215, 179530, 204946, 177065,\n",
       "        11472,  54241,  88143,   3451, 214274, 201906,  54241, 206693,\n",
       "       170947, 187791, 187256, 195563, 161893, 209535, 221002, 170776,\n",
       "         8339, 227122,  54241, 197785, 166270, 222844,   8339, 168703,\n",
       "       191726, 191585, 225645, 175743], dtype=int32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', ':', 'borden', '@', 'head-cfa.harvard.edu', '(', 'Dave', 'Borden', ')', 'Subject', ':', 'Drug', 'Use', 'and', 'Policy', 'in', 'Japan', 'Is', 'anyone', 'out', 'there', 'knowledgeable', 'on', 'drug', 'issues', 'in', 'Japan', '?', 'I', \"'m\", 'interested', 'in', 'knowing', 'if', 'Japan', 'has', 'or', 'has', 'ever', 'had', 'a', 'problem', 'with', 'drugs', ',', 'and', 'how', 'they', 'dealt', 'with', 'it', '.', 'I', \"'ve\", 'heard', ',', 'undocumented', ',', 'that', 'Japan', 'years', 'ago', 'used', 'heavy', 'legal', 'penalties', 'to', 'end', 'a', 'serious', 'heroin', 'problem', '.', 'I', \"'d\", 'like', 'to', 'know', 'both', 'sides', 'of', 'the', 'story', '.', 'Does', 'anyone', 'recall', 'such', 'a', 'problem', '?', 'What', 'were', 'laws', 'at', 'the', 'time', 'relating', 'to', 'drug']\n",
      "[ 80755  51520 167770  54243 187440   3510  72772  63205   3511 135215\n",
      "  51520  74183 142232 163394 122097 189900  93404  91343 163868 203332\n",
      " 221149 193540 202623 178724 191706 189900  93404  54242  88143   3043\n",
      " 191081 189900 193534 189380  93404 187256 203004 187256 181127 186770\n",
      " 161091 207623 227599 178735   8339 163394 188648 221215 175658 227599\n",
      " 191726  11472  88143   3451 187506   8339 224020   8339 221002  93404\n",
      " 228858 162410 224820 187567 194392 204893 221868 180147 161091 214723\n",
      " 187791 207623  11472  88143   2753 194833 221868 193523 167828 215521\n",
      " 202326 221019 218471  11472  73891 163868 210168 219094 161091 207623\n",
      "  54242 147933 226964 194149 164960 221019 221667 210824 221868 178724\n",
      " 224812   8339 178724 175654   8339 163394 178724 222350  54242 147933\n",
      " 164360 221019 194149 201740  54242 147933 203270 163730 197124   8339\n",
      " 194833 179457 163394 222608 187256  93404 224820  54242  87896 164360\n",
      " 178735 210637 168792 221019  93406 204946  54242  87896 179535 187342\n",
      " 163730 197124 166397 189900  93404  54242 138532 183529 228991 187674\n",
      "  11472   8781  72785  63205 167770  54243 195855      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "[1. 0. 0. 0.]\n",
      "(13108, 500)\n",
      "(13108, 4)\n"
     ]
    }
   ],
   "source": [
    "# print(groups)\n",
    "outp = to_categorical(groups)\n",
    "print(words[1][:100])\n",
    "print(input_data[1])\n",
    "print(outp[1])\n",
    "print(input_data.shape)\n",
    "print(outp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11797\n"
     ]
    }
   ],
   "source": [
    "train_num = int(outp.shape[0] * .9)\n",
    "print(train_num)\n",
    "\n",
    "input_train = input_data[:train_num]\n",
    "outp_train = outp[:train_num]\n",
    "\n",
    "input_test = input_data[train_num+1:]\n",
    "outp_test = outp[train_num+1:]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 500\n",
    "    \n",
    "modelQ5 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    \n",
    "    tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                        activation = 'sigmoid',\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    \n",
    "    tf.keras.layers.Dense(4, activation = 'softmax')\n",
    "  \n",
    "])\n",
    "    \n",
    "modelQ5.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, None, 256)         58924288  \n",
      "_________________________________________________________________\n",
      "simple_rnn_12 (SimpleRNN)    (None, 500)               378500    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 59,304,792\n",
      "Trainable params: 59,304,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelQ5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11797 samples, validate on 1310 samples\n",
      "Epoch 1/50\n",
      "11797/11797 [==============================] - 267s 23ms/sample - loss: 1.3339 - acc: 0.3488 - val_loss: 1.2237 - val_acc: 0.2237\n",
      "Epoch 2/50\n",
      "11797/11797 [==============================] - 269s 23ms/sample - loss: 1.3082 - acc: 0.3670 - val_loss: 0.9552 - val_acc: 0.7115\n",
      "Epoch 3/50\n",
      "11797/11797 [==============================] - 269s 23ms/sample - loss: 1.2721 - acc: 0.3925 - val_loss: 1.0292 - val_acc: 0.7198\n",
      "Epoch 4/50\n",
      "11797/11797 [==============================] - 269s 23ms/sample - loss: 1.2165 - acc: 0.4124 - val_loss: 1.2532 - val_acc: 0.2382\n",
      "Epoch 5/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1592 - acc: 0.4365 - val_loss: 1.3500 - val_acc: 0.2420\n",
      "Epoch 6/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1324 - acc: 0.4538 - val_loss: 1.3430 - val_acc: 0.2389\n",
      "Epoch 7/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1126 - acc: 0.4538 - val_loss: 1.1852 - val_acc: 0.7176\n",
      "Epoch 8/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1348 - acc: 0.4494 - val_loss: 1.1946 - val_acc: 0.7145\n",
      "Epoch 9/50\n",
      "11797/11797 [==============================] - 269s 23ms/sample - loss: 1.1161 - acc: 0.4495 - val_loss: 1.2034 - val_acc: 0.7168\n",
      "Epoch 10/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1035 - acc: 0.4628 - val_loss: 1.2693 - val_acc: 0.2351\n",
      "Epoch 11/50\n",
      "11797/11797 [==============================] - 271s 23ms/sample - loss: 1.1002 - acc: 0.4566 - val_loss: 1.2931 - val_acc: 0.7137\n",
      "Epoch 12/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1212 - acc: 0.4529 - val_loss: 1.3919 - val_acc: 0.2313\n",
      "Epoch 13/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1040 - acc: 0.4538 - val_loss: 1.2318 - val_acc: 0.7130\n",
      "Epoch 14/50\n",
      "11797/11797 [==============================] - 271s 23ms/sample - loss: 1.0969 - acc: 0.4631 - val_loss: 1.4288 - val_acc: 0.2359\n",
      "Epoch 15/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1050 - acc: 0.4583 - val_loss: 1.2210 - val_acc: 0.7137\n",
      "Epoch 16/50\n",
      "11797/11797 [==============================] - 271s 23ms/sample - loss: 1.1025 - acc: 0.4634 - val_loss: 1.3868 - val_acc: 0.2321\n",
      "Epoch 17/50\n",
      "11797/11797 [==============================] - 283s 24ms/sample - loss: 1.0953 - acc: 0.4634 - val_loss: 1.3905 - val_acc: 0.2359\n",
      "Epoch 18/50\n",
      "11797/11797 [==============================] - 278s 24ms/sample - loss: 1.0945 - acc: 0.4590 - val_loss: 1.4127 - val_acc: 0.7107\n",
      "Epoch 19/50\n",
      "11797/11797 [==============================] - 281s 24ms/sample - loss: 1.0917 - acc: 0.4671 - val_loss: 1.3262 - val_acc: 0.2359\n",
      "Epoch 20/50\n",
      "11797/11797 [==============================] - 283s 24ms/sample - loss: 1.0953 - acc: 0.4581 - val_loss: 1.2574 - val_acc: 0.7153\n",
      "Epoch 21/50\n",
      "11797/11797 [==============================] - 272s 23ms/sample - loss: 1.0978 - acc: 0.4558 - val_loss: 1.3725 - val_acc: 0.7168\n",
      "Epoch 22/50\n",
      "11797/11797 [==============================] - 277s 23ms/sample - loss: 1.0903 - acc: 0.4644 - val_loss: 1.2997 - val_acc: 0.7137\n",
      "Epoch 23/50\n",
      "11797/11797 [==============================] - 280s 24ms/sample - loss: 1.0913 - acc: 0.4713 - val_loss: 1.3439 - val_acc: 0.7130\n",
      "Epoch 24/50\n",
      "11797/11797 [==============================] - 278s 24ms/sample - loss: 1.1454 - acc: 0.4402 - val_loss: 1.2169 - val_acc: 0.7153\n",
      "Epoch 25/50\n",
      "11797/11797 [==============================] - 286s 24ms/sample - loss: 1.1135 - acc: 0.4558 - val_loss: 1.3638 - val_acc: 0.7176\n",
      "Epoch 26/50\n",
      "11797/11797 [==============================] - 274s 23ms/sample - loss: 1.1004 - acc: 0.4637 - val_loss: 1.4039 - val_acc: 0.2382\n",
      "Epoch 27/50\n",
      "11797/11797 [==============================] - 275s 23ms/sample - loss: 1.0962 - acc: 0.4625 - val_loss: 1.3687 - val_acc: 0.2389\n",
      "Epoch 28/50\n",
      "11797/11797 [==============================] - 284s 24ms/sample - loss: 1.0925 - acc: 0.4599 - val_loss: 1.3770 - val_acc: 0.7160\n",
      "Epoch 29/50\n",
      "11797/11797 [==============================] - 272s 23ms/sample - loss: 1.0907 - acc: 0.4672 - val_loss: 1.2951 - val_acc: 0.7084\n",
      "Epoch 30/50\n",
      "11797/11797 [==============================] - 272s 23ms/sample - loss: 1.0954 - acc: 0.4630 - val_loss: 1.4036 - val_acc: 0.7160\n",
      "Epoch 31/50\n",
      "11797/11797 [==============================] - 272s 23ms/sample - loss: 1.0892 - acc: 0.4660 - val_loss: 1.3404 - val_acc: 0.7153\n",
      "Epoch 32/50\n",
      "11797/11797 [==============================] - 272s 23ms/sample - loss: 1.0933 - acc: 0.4706 - val_loss: 1.2579 - val_acc: 0.7160\n",
      "Epoch 33/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1130 - acc: 0.4534 - val_loss: 1.3327 - val_acc: 0.2366\n",
      "Epoch 34/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.0989 - acc: 0.4602 - val_loss: 1.3516 - val_acc: 0.2374\n",
      "Epoch 35/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.0973 - acc: 0.4618 - val_loss: 1.3508 - val_acc: 0.7160\n",
      "Epoch 36/50\n",
      "11797/11797 [==============================] - 271s 23ms/sample - loss: 1.0964 - acc: 0.4599 - val_loss: 1.3893 - val_acc: 0.2366\n",
      "Epoch 37/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.0949 - acc: 0.4642 - val_loss: 1.3843 - val_acc: 0.7145\n",
      "Epoch 38/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.0949 - acc: 0.4627 - val_loss: 1.4239 - val_acc: 0.7191\n",
      "Epoch 39/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.0983 - acc: 0.4661 - val_loss: 1.3672 - val_acc: 0.7191\n",
      "Epoch 40/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.0933 - acc: 0.4669 - val_loss: 1.4323 - val_acc: 0.7137\n",
      "Epoch 41/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1167 - acc: 0.4612 - val_loss: 1.2279 - val_acc: 0.7176\n",
      "Epoch 42/50\n",
      "11797/11797 [==============================] - 270s 23ms/sample - loss: 1.1017 - acc: 0.4654 - val_loss: 1.3821 - val_acc: 0.2382\n",
      "Epoch 43/50\n",
      "11797/11797 [==============================] - 276s 23ms/sample - loss: 1.0973 - acc: 0.4643 - val_loss: 1.3059 - val_acc: 0.7206\n",
      "Epoch 44/50\n",
      "11797/11797 [==============================] - 297s 25ms/sample - loss: 1.0953 - acc: 0.4628 - val_loss: 1.2830 - val_acc: 0.7191\n",
      "Epoch 45/50\n",
      "11797/11797 [==============================] - 297s 25ms/sample - loss: 1.0909 - acc: 0.4660 - val_loss: 1.2843 - val_acc: 0.7183\n",
      "Epoch 46/50\n",
      "11797/11797 [==============================] - 284s 24ms/sample - loss: 1.0896 - acc: 0.4650 - val_loss: 1.3264 - val_acc: 0.7191\n",
      "Epoch 47/50\n",
      "11797/11797 [==============================] - 285s 24ms/sample - loss: 1.0896 - acc: 0.4662 - val_loss: 1.3732 - val_acc: 0.7206\n",
      "Epoch 48/50\n",
      "11797/11797 [==============================] - 273s 23ms/sample - loss: 1.0887 - acc: 0.4598 - val_loss: 1.4265 - val_acc: 0.7168\n",
      "Epoch 49/50\n",
      "11797/11797 [==============================] - 278s 24ms/sample - loss: 1.0925 - acc: 0.4674 - val_loss: 1.3406 - val_acc: 0.7137\n",
      "Epoch 50/50\n",
      "11797/11797 [==============================] - 293s 25ms/sample - loss: 1.0859 - acc: 0.4679 - val_loss: 1.3090 - val_acc: 0.7183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0b4466ee48>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modelQ5.fit(input_train, outp_train, epochs=50, batch_size = 64, validation_data=(input_test, outp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelQ5.save(\"modelQ5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
